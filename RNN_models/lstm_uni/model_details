 comments: dip_imu_nn training on birnnBiRNN(
  (relu): ReLU()
  (pre_fc): Linear(in_features=20, out_features=256, bias=True)
  (lstm): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.3)
  (post_fc): Linear(in_features=256, out_features=60, bias=True)
  (dropout): Dropout(p=0.3, inplace=False)
)
no of batches-- 86 
epoch no ----------> 0 training loss 1.185592532157898 
epoch no ----------> 0 validation loss 1.1682859659194946 
epoch no ----------> 1 training loss 0.8984155654907227 
epoch no ----------> 1 validation loss 1.1398506164550781 
epoch no ----------> 2 training loss 0.8495129346847534 
epoch no ----------> 2 validation loss 1.0539623498916626 
epoch no ----------> 3 training loss 0.8063963055610657 
epoch no ----------> 3 validation loss 1.032554268836975 
epoch no ----------> 4 training loss 0.7716463804244995 
epoch no ----------> 4 validation loss 1.0387321710586548 
epoch no ----------> 5 training loss 0.739270031452179 
epoch no ----------> 5 validation loss 0.9570264220237732 
epoch no ----------> 6 training loss 0.7112739682197571 
epoch no ----------> 6 validation loss 0.9622661471366882 
epoch no ----------> 7 training loss 0.6956971287727356 
epoch no ----------> 7 validation loss 0.8955927491188049 
epoch no ----------> 8 training loss 0.6686721444129944 
epoch no ----------> 8 validation loss 0.8539134860038757 
epoch no ----------> 9 training loss 0.651580274105072 
epoch no ----------> 9 validation loss 0.8236362338066101 
epoch no ----------> 10 training loss 0.6373415589332581 
epoch no ----------> 10 validation loss 0.816552460193634 
epoch no ----------> 11 training loss 0.622106671333313 
epoch no ----------> 11 validation loss 0.798943042755127 
epoch no ----------> 12 training loss 0.618162989616394 
epoch no ----------> 12 validation loss 0.8029361367225647 
epoch no ----------> 13 training loss 0.6011797189712524 
epoch no ----------> 13 validation loss 0.8057273030281067 
epoch no ----------> 14 training loss 0.5939686298370361 
epoch no ----------> 14 validation loss 0.8323206901550293 
epoch no ----------> 15 training loss 0.5775448679924011 
epoch no ----------> 15 validation loss 0.819655179977417 
epoch no ----------> 16 training loss 0.5799088478088379 
epoch no ----------> 16 validation loss 0.8399620056152344 
epoch no ----------> 17 training loss 0.5709039568901062 
epoch no ----------> 17 validation loss 0.8111192584037781 
epoch no ----------> 18 training loss 0.5741004347801208 
epoch no ----------> 18 validation loss 0.7915975451469421 
epoch no ----------> 19 training loss 0.5568391680717468 
epoch no ----------> 19 validation loss 0.7878063321113586 
epoch no ----------> 20 training loss 0.5527800917625427 
epoch no ----------> 20 validation loss 0.7753843665122986 
epoch no ----------> 21 training loss 0.5389043092727661 
epoch no ----------> 21 validation loss 0.7726507782936096 
epoch no ----------> 22 training loss 0.5314347743988037 
epoch no ----------> 22 validation loss 0.7386109232902527 
epoch no ----------> 23 training loss 0.5265644788742065 
epoch no ----------> 23 validation loss 0.7628782391548157 
epoch no ----------> 24 training loss 0.5161579251289368 
epoch no ----------> 24 validation loss 0.8480426669120789 
epoch no ----------> 25 training loss 0.5214547514915466 
epoch no ----------> 25 validation loss 0.7409067749977112 
epoch no ----------> 26 training loss 0.5105361938476562 
epoch no ----------> 26 validation loss 0.7800982594490051 
epoch no ----------> 27 training loss 0.4971235394477844 
epoch no ----------> 27 validation loss 0.7455727458000183 
epoch no ----------> 28 training loss 0.49032503366470337 
epoch no ----------> 28 validation loss 0.7578060030937195 
epoch no ----------> 29 training loss 0.4883413016796112 
epoch no ----------> 29 validation loss 0.7405306696891785 
